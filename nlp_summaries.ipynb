{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e906663b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2f126",
   "metadata": {},
   "source": [
    "# Movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b842b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARACTER_DATA_PATH = './data/character.metadata.tsv'\n",
    "MOVIE_DATA_PATH = './data/movie.metadata.tsv'\n",
    "SUMMARIES_DATA_PATH = './data/plot_summaries.txt'\n",
    "NAME_DATA_PATH = './data/name.clusters.txt'\n",
    "TYPE_DATA_PATH = './data/tvtropes.clusters.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bae4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOVIE_HEADER = ['Wikipedia_movie_ID','Freebase_movie_ID','Movie_name','Movie_release_date',\n",
    "                'Movie_box_office_revenue','Movie_runtime','Movie_languages','Movie_countries','Movie_genres']\n",
    "\n",
    "movie = pd.read_table(MOVIE_DATA_PATH,header=None,names=MOVIE_HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06094a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has already been parsed and modified.\n",
      "Number of US movies : 34408\n"
     ]
    }
   ],
   "source": [
    "def format_dict(x):\n",
    "    n = len(x)\n",
    "    if n==0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return ','.join(str(s) for s in list(x.values()))\n",
    "    \n",
    "\n",
    "try :\n",
    "\n",
    "    movie['Movie_genres'] = movie['Movie_genres'].apply(json.loads).apply(format_dict)\n",
    "    movie['Movie_countries'] = movie['Movie_countries'].apply(json.loads).apply(format_dict)\n",
    "    movie['Movie_languages'] = movie['Movie_languages'].apply(json.loads).apply(format_dict)\n",
    "except json.decoder.JSONDecodeError:\n",
    "    print('Data has already been parsed and modified.')\n",
    "    \n",
    "# Keep only American movies\n",
    "us_movies = movie[movie['Movie_countries'].astype(str).str.contains('United States of America')]\n",
    "print(\"Number of US movies : {}\".format(len(us_movies)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda655cf",
   "metadata": {},
   "source": [
    "# Summaries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f37d8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21354</th>\n",
       "      <td>23437432</td>\n",
       "      <td>Following the multiple-Aema theme started in M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>3151857</td>\n",
       "      <td>Leonard Helperman is a 4th grader whose mother...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20874</th>\n",
       "      <td>3657826</td>\n",
       "      <td>Sam Dalmas  is an American writer currently li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15311</th>\n",
       "      <td>4894283</td>\n",
       "      <td>A single eventful night in the lives of a crew...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33876</th>\n",
       "      <td>6832750</td>\n",
       "      <td>Dr. Alice Dodgson  gets her medical license re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Wikipedia_movie_ID                                            Summary\n",
       "21354            23437432  Following the multiple-Aema theme started in M...\n",
       "9162              3151857  Leonard Helperman is a 4th grader whose mother...\n",
       "20874             3657826  Sam Dalmas  is an American writer currently li...\n",
       "15311             4894283  A single eventful night in the lives of a crew...\n",
       "33876             6832750  Dr. Alice Dodgson  gets her medical license re..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = pd.read_table(SUMMARIES_DATA_PATH,header=None,names=['Wikipedia_movie_ID','Summary'])\n",
    "summaries.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a72a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wikipedia_movie_ID</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Wikipedia_movie_ID, Summary]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[summaries[\"Summary\"]=='NaN']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e49fa",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ab3dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of US movies with summary : 20788\n"
     ]
    }
   ],
   "source": [
    "us_summaries = us_movies.merge(summaries, how='inner', on=['Wikipedia_movie_ID'])\n",
    "print(\"Number of US movies with summary : {}\".format(len(us_summaries)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912da81",
   "metadata": {},
   "source": [
    "# Vectorization of summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be32a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for transformers<5.0.0,>=4.6.0 from https://files.pythonhosted.org/packages/9a/06/e4ec2a321e57c03b7e9345d709d554a52c33760e5015fdff0919d9459af0/transformers-4.35.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
      "     ---------------------------------------- 0.0/123.1 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/123.1 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/123.1 kB ? eta -:--:--\n",
      "     ------------ ------------------------ 41.0/123.1 kB 279.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 123.1/123.1 kB 718.6 kB/s eta 0:00:00\n",
      "Collecting tqdm (from sentence_transformers)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting torch>=1.6.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for torch>=1.6.0 from https://files.pythonhosted.org/packages/67/0a/b6dddafbb64d3ca13078a2616a2ea02c595da832586898a7eb414cf7ad10/torch-2.1.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading torch-2.1.0-cp39-cp39-win_amd64.whl.metadata (24 kB)\n",
      "Collecting torchvision (from sentence_transformers)\n",
      "  Obtaining dependency information for torchvision from https://files.pythonhosted.org/packages/ec/36/1ecc19249def521b3b948baee32903148b1f399d2dd5a9a5692942e8383c/torchvision-0.16.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading torchvision-0.16.0-cp39-cp39-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from sentence_transformers) (1.25.2)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/1c/49/30ffcac5af06d08dfdd27da322ce31a373b733711bb272941877c1e4794a/scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from sentence_transformers) (1.11.1)\n",
      "Collecting nltk (from sentence_transformers)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 0.5/1.5 MB 9.4 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 1.1/1.5 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 12.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.9 MB/s eta 0:00:00\n",
      "Collecting sentencepiece (from sentence_transformers)\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.6 kB ? eta -:--:--\n",
      "     ------------------------------------  972.8/977.6 kB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- 977.6/977.6 kB 10.3 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.4.0 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.4.0->sentence_transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/84/4d/82704d1ab9290b03da94e6425f5e87396b999fd7eb8e08f3a92c158402bf/PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
      "Collecting sympy (from torch>=1.6.0->sentence_transformers)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ---------------------------------------- 0.0/5.7 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 1.5/5.7 MB 47.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 3.5/5.7 MB 36.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 5.0/5.7 MB 35.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.7/5.7 MB 36.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.7/5.7 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting networkx (from torch>=1.6.0->sentence_transformers)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from tqdm->sentence_transformers) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fc/85/0d1038f068900896a8590d6d0da198b90d31f731a39166a432aa2b92249b/regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/36/de/de1b1d7b191821cc2e6e84251cf9641e4fbd205fa5ec816d52fe42f97325/tokenizers-0.14.1-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.14.1-cp39-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/a7/49/d43e967d427e78086980ec85af1afe0320becfc33b26903d3d0ce10b0ee5/safetensors-0.4.0-cp39-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.0-cp39-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting click (from nltk->sentence_transformers)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk->sentence_transformers)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/10/40/d551139c85db202f1f384ba8bcf96aca2f329440a844f924c8a0040b6d02/joblib-1.3.2-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->sentence_transformers)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from torchvision->sentence_transformers) (9.4.0)\n",
      "Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
      "  Obtaining dependency information for huggingface-hub>=0.4.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.6.0->sentence_transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ---------------------------------------  532.5/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 11.2 MB/s eta 0:00:00\n",
      "Downloading torch-2.1.0-cp39-cp39-win_amd64.whl (192.2 MB)\n",
      "   ---------------------------------------- 0.0/192.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.8/192.2 MB 56.8 MB/s eta 0:00:04\n",
      "    --------------------------------------- 4.5/192.2 MB 47.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 6.3/192.2 MB 50.3 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 8.3/192.2 MB 53.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 11.1/192.2 MB 50.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 14.4/192.2 MB 46.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 17.3/192.2 MB 50.1 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 20.5/192.2 MB 65.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 22.5/192.2 MB 54.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 25.0/192.2 MB 21.9 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 27.5/192.2 MB 21.1 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 30.2/192.2 MB 21.8 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 32.9/192.2 MB 50.4 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 35.7/192.2 MB 50.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 38.6/192.2 MB 54.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 41.4/192.2 MB 54.7 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 41.7/192.2 MB 43.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 44.0/192.2 MB 43.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 47.2/192.2 MB 46.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 49.4/192.2 MB 43.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 52.7/192.2 MB 59.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 57.2/192.2 MB 59.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 60.2/192.2 MB 72.6 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 63.5/192.2 MB 73.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 67.0/192.2 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 69.9/192.2 MB 59.5 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 73.3/192.2 MB 65.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 76.2/192.2 MB 65.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 77.9/192.2 MB 54.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 78.6/192.2 MB 50.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 80.3/192.2 MB 46.7 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 82.2/192.2 MB 40.9 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 83.3/192.2 MB 36.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 85.5/192.2 MB 34.4 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 87.8/192.2 MB 32.8 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 90.7/192.2 MB 43.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 93.7/192.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 95.9/192.2 MB 50.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 98.9/192.2 MB 59.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 101.1/192.2 MB 59.5 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 104.0/192.2 MB 54.4 MB/s eta 0:00:02\n",
      "   --------------------- ----------------- 106.5/192.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 109.3/192.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ---------------------- ---------------- 112.2/192.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 114.4/192.2 MB 54.4 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 116.9/192.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 119.6/192.2 MB 50.4 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 122.8/192.2 MB 54.7 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 125.3/192.2 MB 50.4 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 128.8/192.2 MB 65.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 132.1/192.2 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 135.0/192.2 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 137.4/192.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 140.7/192.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 143.1/192.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 145.8/192.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 148.3/192.2 MB 59.8 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 150.8/192.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 153.3/192.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 156.3/192.2 MB 59.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 159.1/192.2 MB 54.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 159.9/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 162.2/192.2 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 165.4/192.2 MB 50.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 168.8/192.2 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 171.0/192.2 MB 59.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 173.3/192.2 MB 65.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 174.7/192.2 MB 54.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 175.6/192.2 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 178.3/192.2 MB 43.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 180.9/192.2 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 183.6/192.2 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 186.8/192.2 MB 54.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  188.9/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  191.0/192.2 MB 50.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  192.2/192.2 MB 54.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- 192.2/192.2 MB 648.1 kB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.3 MB/s eta 0:00:00\n",
      "Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 0.0/7.9 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 2.3/7.9 MB 75.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 4.9/7.9 MB 62.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.1/7.9 MB 64.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 63.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  7.9/7.9 MB 63.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.9/7.9 MB 31.6 MB/s eta 0:00:00\n",
      "Downloading scikit_learn-1.3.2-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.3/9.3 MB 70.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.2/9.3 MB 66.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.5/9.3 MB 60.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 59.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 59.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.3/9.3 MB 59.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 31.4 MB/s eta 0:00:00\n",
      "Downloading torchvision-0.16.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.3/1.3 MB 83.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 27.2 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "   ---------------------------------------- 0.0/166.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 166.4/166.4 kB 10.4 MB/s eta 0:00:00\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "   ---------------------------------------- 0.0/302.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.2/302.2 kB 9.4 MB/s eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp39-cp39-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.8/152.8 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.0-cp39-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 277.2/277.2 kB 17.8 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading tokenizers-0.14.1-cp39-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.0/2.2 MB 134.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 295.0/295.0 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.6/1.6 MB 108.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.6/1.6 MB 108.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 17.6 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: sentence_transformers\n",
      "  Building wheel for sentence_transformers (setup.py): started\n",
      "  Building wheel for sentence_transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125960 sha256=c771cced55bddeb677d188f7962bff877e8928a63332c78f54ffe395756febb5\n",
      "  Stored in directory: c:\\users\\beynes\\appdata\\local\\pip\\cache\\wheels\\71\\67\\06\\162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "Successfully built sentence_transformers\n",
      "Installing collected packages: sentencepiece, mpmath, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, networkx, joblib, fsspec, filelock, click, torch, scikit-learn, nltk, huggingface-hub, torchvision, tokenizers, transformers, sentence_transformers\n",
      "Successfully installed click-8.1.7 filelock-3.13.1 fsspec-2023.10.0 huggingface-hub-0.17.3 joblib-1.3.2 mpmath-1.3.0 networkx-3.2.1 nltk-3.8.1 pyyaml-6.0.1 regex-2023.10.3 safetensors-0.4.0 scikit-learn-1.3.2 sentence_transformers-2.2.2 sentencepiece-0.1.99 sympy-1.12 threadpoolctl-3.2.0 tokenizers-0.14.1 torch-2.1.0 torchvision-0.16.0 tqdm-4.66.1 transformers-4.35.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textract\n",
      "  Using cached textract-1.6.5-py3-none-any.whl (23 kB)\n",
      "Collecting argcomplete~=1.10.0 (from textract)\n",
      "  Using cached argcomplete-1.10.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting beautifulsoup4~=4.8.0 (from textract)\n",
      "  Using cached beautifulsoup4-4.8.2-py3-none-any.whl (106 kB)\n",
      "Collecting chardet==3.* (from textract)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "     ---------------------------------------- 0.0/133.4 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/133.4 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/133.4 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/133.4 kB 245.8 kB/s eta 0:00:01\n",
      "     -----------------------------------  133.1/133.4 kB 787.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 133.4/133.4 kB 658.4 kB/s eta 0:00:00\n",
      "Collecting docx2txt~=0.8 (from textract)\n",
      "  Using cached docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting extract-msg<=0.29.* (from textract)\n",
      "  Using cached extract_msg-0.28.7-py2.py3-none-any.whl (69 kB)\n",
      "Collecting pdfminer.six==20191110 (from textract)\n",
      "  Using cached pdfminer.six-20191110-py2.py3-none-any.whl (5.6 MB)\n",
      "Collecting python-pptx~=0.6.18 (from textract)\n",
      "  Obtaining dependency information for python-pptx~=0.6.18 from https://files.pythonhosted.org/packages/72/49/6eee83072983473e9905ffddd5c2032b9a0ca4616425560d6d582287b467/python_pptx-0.6.23-py3-none-any.whl.metadata\n",
      "  Downloading python_pptx-0.6.23-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting six~=1.12.0 (from textract)\n",
      "  Using cached six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting SpeechRecognition~=3.8.1 (from textract)\n",
      "  Using cached SpeechRecognition-3.8.1-py2.py3-none-any.whl (32.8 MB)\n",
      "Collecting xlrd~=1.2.0 (from textract)\n",
      "  Downloading xlrd-1.2.0-py2.py3-none-any.whl (103 kB)\n",
      "     ---------------------------------------- 0.0/103.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 103.3/103.3 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting pycryptodome (from pdfminer.six==20191110->textract)\n",
      "  Obtaining dependency information for pycryptodome from https://files.pythonhosted.org/packages/87/c4/c979db0914a23541d62c9e4b5e8a30f56a78c6dec8677db6a5327d306be5/pycryptodome-3.19.0-cp35-abi3-win_amd64.whl.metadata\n",
      "  Downloading pycryptodome-3.19.0-cp35-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting sortedcontainers (from pdfminer.six==20191110->textract)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from beautifulsoup4~=4.8.0->textract) (2.4)\n",
      "Collecting imapclient==2.1.0 (from extract-msg<=0.29.*->textract)\n",
      "  Using cached IMAPClient-2.1.0-py2.py3-none-any.whl (73 kB)\n",
      "Collecting olefile>=0.46 (from extract-msg<=0.29.*->textract)\n",
      "  Downloading olefile-0.46.zip (112 kB)\n",
      "     ---------------------------------------- 0.0/112.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 112.2/112.2 kB 3.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tzlocal>=2.1 (from extract-msg<=0.29.*->textract)\n",
      "  Obtaining dependency information for tzlocal>=2.1 from https://files.pythonhosted.org/packages/97/3f/c4c51c55ff8487f2e6d0e618dba917e3c3ee2caae6cf0fbb59c9b1876f2e/tzlocal-5.2-py3-none-any.whl.metadata\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting compressed-rtf>=1.0.6 (from extract-msg<=0.29.*->textract)\n",
      "  Using cached compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting ebcdic>=1.1.1 (from extract-msg<=0.29.*->textract)\n",
      "  Using cached ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from python-pptx~=0.6.18->textract) (4.9.3)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx~=0.6.18->textract)\n",
      "  Obtaining dependency information for XlsxWriter>=0.5.7 from https://files.pythonhosted.org/packages/f7/3e/05ba2194cd5073602422859c949a4f21310a3c49bf8dccde9e03d4522b11/XlsxWriter-3.1.9-py3-none-any.whl.metadata\n",
      "  Downloading XlsxWriter-3.1.9-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: tzdata in c:\\users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (2023.3)\n",
      "Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
      "   ---------------------------------------- 0.0/471.6 kB ? eta -:--:--\n",
      "   ---------------------------------------  471.0/471.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 471.6/471.6 kB 9.8 MB/s eta 0:00:00\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
      "   ---------------------------------------- 0.0/154.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 154.8/154.8 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading pycryptodome-3.19.0-cp35-abi3-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.7/1.7 MB 107.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.7/1.7 MB 107.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 12.3 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: docx2txt, compressed-rtf, olefile\n",
      "  Building wheel for docx2txt (setup.py): started\n",
      "  Building wheel for docx2txt (setup.py): finished with status 'done'\n",
      "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3973 sha256=56b169e95d266073e96786d6d12ff20bc74377cbf2d70f9262662beddb288359\n",
      "  Stored in directory: c:\\users\\beynes\\appdata\\local\\pip\\cache\\wheels\\40\\75\\01\\e6c444034338bde9c7947d3467807f889123465c2371e77418\n",
      "  Building wheel for compressed-rtf (setup.py): started\n",
      "  Building wheel for compressed-rtf (setup.py): finished with status 'done'\n",
      "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6197 sha256=d93506c036eadb14c1b0c6bb9ae054acd63498bb5779f4c1eba565266ba4c4e0\n",
      "  Stored in directory: c:\\users\\beynes\\appdata\\local\\pip\\cache\\wheels\\e4\\67\\e4\\ba2159853bdd0fe99330aa1e384915108143a5370686ea446f\n",
      "  Building wheel for olefile (setup.py): started\n",
      "  Building wheel for olefile (setup.py): finished with status 'done'\n",
      "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35472 sha256=b44b66837364100a5bf3dc1f5362880efe304e953d1891ae37de44c85c4f1d57\n",
      "  Stored in directory: c:\\users\\beynes\\appdata\\local\\pip\\cache\\wheels\\64\\b8\\ba\\ebba30390fbd997074f35e42a842ce3fd933213cac8753414e\n",
      "Successfully built docx2txt compressed-rtf olefile\n",
      "Installing collected packages: SpeechRecognition, sortedcontainers, ebcdic, docx2txt, compressed-rtf, chardet, argcomplete, XlsxWriter, xlrd, tzlocal, six, pycryptodome, olefile, beautifulsoup4, python-pptx, pdfminer.six, imapclient, extract-msg, textract\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: beautifulsoup4\n",
      "    Found existing installation: beautifulsoup4 4.12.2\n",
      "    Uninstalling beautifulsoup4-4.12.2:\n",
      "      Successfully uninstalled beautifulsoup4-4.12.2\n",
      "Successfully installed SpeechRecognition-3.8.1 XlsxWriter-3.1.9 argcomplete-1.10.3 beautifulsoup4-4.8.2 chardet-3.0.4 compressed-rtf-1.0.6 docx2txt-0.8 ebcdic-1.1.1 extract-msg-0.28.7 imapclient-2.1.0 olefile-0.46 pdfminer.six-20191110 pycryptodome-3.19.0 python-pptx-0.6.23 six-1.12.0 sortedcontainers-2.4.0 textract-1.6.5 tzlocal-5.2 xlrd-1.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence_transformers\n",
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eaf82b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beynes\\anaconda3\\envs\\ada\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import textract\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0a3516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\beynes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8305b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6739f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(text):\n",
    "    # Input : the whole text as one string\n",
    "    # Output : mean vector of all the embeddings vectors related to the sentences of the text input, as well as all embeddings\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = text.replace(u'\\xa0', u' ')\n",
    "    text = text.replace(u'\\xc2','')\n",
    "    # Split the whole text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Embedding using BERT model (can convert a sentence into a vector of dimension 768)\n",
    "    df_text_sents = pd.Series(sentences)\n",
    "    df_text_embeddings = df_text_sents.map(lambda x: model.encode(x))\n",
    "    # Convert all vectors from reference text into one single vector, by taking the mean\n",
    "    mean_vector = df_text_embeddings.sum() / df_text_embeddings.size\n",
    "    \n",
    "    return(mean_vector,df_text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dfe5fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                            | 16/20788 [00:40<14:43:43,  2.55s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sentence_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m tqdm(sentences):\n\u001b[1;32m----> 6\u001b[0m     sentence_embeddings\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtext2vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m us_summaries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmbedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sentence_embeddings\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtext2vec\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Embedding using BERT model (can convert a sentence into a vector of dimension 768)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_text_sents \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(sentences)\n\u001b[1;32m---> 12\u001b[0m df_text_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mdf_text_sents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert all vectors from reference text into one single vector, by taking the mean\u001b[39;00m\n\u001b[0;32m     14\u001b[0m mean_vector \u001b[38;5;241m=\u001b[39m df_text_embeddings\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m df_text_embeddings\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\series.py:4397\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\n\u001b[0;32m   4319\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4320\u001b[0m     arg: Callable \u001b[38;5;241m|\u001b[39m Mapping \u001b[38;5;241m|\u001b[39m Series,\n\u001b[0;32m   4321\u001b[0m     na_action: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4322\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4324\u001b[0m \u001b[38;5;124;03m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4325\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4395\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   4396\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4397\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_values, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4399\u001b[0m         \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4400\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\core\\base.py:924\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m# mapper is a function\u001b[39;00m\n\u001b[1;32m--> 924\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mmap_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_values\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mtext2vec.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Embedding using BERT model (can convert a sentence into a vector of dimension 768)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df_text_sents \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(sentences)\n\u001b[1;32m---> 12\u001b[0m df_text_embeddings \u001b[38;5;241m=\u001b[39m df_text_sents\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Convert all vectors from reference text into one single vector, by taking the mean\u001b[39;00m\n\u001b[0;32m     14\u001b[0m mean_vector \u001b[38;5;241m=\u001b[39m df_text_embeddings\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m df_text_embeddings\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    162\u001b[0m features \u001b[38;5;241m=\u001b[39m batch_to_device(features, device)\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 165\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    168\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     64\u001b[0m     trans_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 66\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     69\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: output_tokens, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:309\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    313\u001b[0m use_cache \u001b[38;5;241m=\u001b[39m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ada\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentences = us_summaries['Summary'].tolist()\n",
    "\n",
    "sentence_embeddings = []\n",
    "\n",
    "for sent in tqdm(sentences):\n",
    "    sentence_embeddings.append(text2vec(sent))\n",
    "\n",
    "us_summaries['Embedding'] = sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacc9c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
